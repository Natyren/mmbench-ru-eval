# mmbench-ru-eval

Repository to simple evaluation your results on MMBench-DEV-RU

## Usage

```bash
python3 evaluate.py --path <path_to_your_predictions> --aggregate <type_of_source_you_want_aggregate_to>
```

## Prerequisite

File that you want to evaluate must have and extra column "predict" with model predictions
